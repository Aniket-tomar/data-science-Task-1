# DATA-SCIENCE-PROJECT TASK-1

*COMPANY*: CODTECH IT SOLUTIONS

 *NAME*: AKSHAT JAIN

*INTERN ID*: CT04DH1930

*DOMAIN*: DATA SCIENCE

*DURATION*: 4 WEEKS

*MENTOR*: 

# ğŸ› ï¸ Data Preprocessing Pipeline using Pandas & Scikit-Learn

## âœ… Task Overview
**Objective:**  
Design and implement an **ETL pipeline (Extract, Transform, Load)** using tools like **Pandas** and **Scikit-Learn**.

**Task Prompt:**  
> *Create a pipeline for data preprocessing, transformation, and loading using tools like Pandas and Scikit-Learn.*

---

## ğŸ“¦ Project Description

This project demonstrates a complete ETL (Extract-Transform-Load) pipeline using the **California Housing dataset**. It includes:

- ğŸ“¥ **Extraction** from sklearnâ€™s built-in datasets  
- ğŸ”§ **Transformation** using preprocessing steps such as feature scaling  
- ğŸ’¾ **Loading** the cleaned and transformed data for future modeling or export

---

## ğŸš€ Pipeline Structure

### 1ï¸âƒ£ Extract
- Load housing dataset using `fetch_california_housing()`

### 2ï¸âƒ£ Transform
- Clean data (e.g. handle missing values)
- Standardize numerical features using `StandardScaler`

### 3ï¸âƒ£ Load
- Save the transformed dataset using `to_csv()`

---

## ğŸ§° Technologies Used

- Python 3.x  
- Pandas  
- Scikit-Learn  
- Google Colab (for interactive workflow)

---

## ğŸ“‚ File Structure
```
.
â”œâ”€â”€ Data_Science_Task_1.ipynb       # Main notebook with ETL pipeline
â”œâ”€â”€ README.md                       # This file
```

---

## ğŸ“¸ Sample Code Snippet
```python
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler

def extract_data():
    data = fetch_california_housing(as_frame=True)
    return data.frame

def transform_data(df):
    scaler = StandardScaler()
    df[df.columns] = scaler.fit_transform(df)
    return df
```

---

## ğŸ§  Key Insights

- Modular functions make the pipeline reusable
- Using Scikit-Learn standard tools makes scaling and transforming easier and faster
- Pipeline is structured and commented for clarity and reusability

---

## âœ… How to Run

1. Clone this repo  
2. Open the notebook `Data_Science_Task_1.ipynb`  
3. Run all cells in order  
4. Export final CSV output if needed
